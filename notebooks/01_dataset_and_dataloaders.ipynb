{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e342095b",
   "metadata": {},
   "source": [
    "# 01 — Dataset & Dataloaders (Places2 — Scene Recognition)\n",
    "\n",
    "This notebook prepares the **Places2 simplified (40 classes)** dataset for training and validation:\n",
    "- builds **train/val split (80/20)**,\n",
    "- applies standard **data augmentations**,\n",
    "- creates **PyTorch DataLoaders**,\n",
    "- saves a small `metadata.json` mapping: class name → idx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2b1bc",
   "metadata": {},
   "source": [
    "> **Expected dataset layout**  \n",
    "> \n",
    "> ```text\n",
    "> /path/to/Places2_simp/\n",
    ">   airport terminal/\n",
    ">     img_0001.jpg\n",
    ">     ...\n",
    ">   amphitheatre/\n",
    ">     ...\n",
    ">   ...\n",
    "> ```\n",
    "> Each subfolder is a class with ~1000 images of size 128×128 (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install numpy pandas matplotlib scikit-learn tqdm\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, utils as tv_utils\n",
    "\n",
    "DATA_ROOT = Path(\"/path/to/Places2_simp\")  # <-- EDIT THIS\n",
    "OUT_DIR = Path(\"../data\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 42\n",
    "BATCH_TRAIN = 256\n",
    "BATCH_VAL = 1024\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "print(\"DATA_ROOT exists:\", DATA_ROOT.exists())\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea0e12",
   "metadata": {},
   "source": [
    "## Transforms & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=20, fill=0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset_full = datasets.ImageFolder(root=str(DATA_ROOT), transform=train_tf)\n",
    "class_to_idx = dataset_full.class_to_idx\n",
    "with open(OUT_DIR / \"metadata.json\", \"w\") as f:\n",
    "    json.dump({\"class_to_idx\": class_to_idx}, f, indent=2)\n",
    "len(dataset_full), len(class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d1a05",
   "metadata": {},
   "source": [
    "## Train/Validation split (80/20, stratified by class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_full))\n",
    "# Build targets via bare folder scan (no transform) to avoid running transforms\n",
    "bare = datasets.ImageFolder(root=str(DATA_ROOT))\n",
    "targets = np.array([y for _, y in bare.imgs])\n",
    "\n",
    "train_idx, val_idx = [], []\n",
    "for cls, idxs in pd.Series(np.arange(len(targets))).groupby(targets).groups.items():\n",
    "    idxs = np.array(list(idxs))\n",
    "    np.random.shuffle(idxs)\n",
    "    n_val = int(0.2 * len(idxs))\n",
    "    val_idx.extend(idxs[:n_val]); train_idx.extend(idxs[n_val:])\n",
    "\n",
    "train_ds = Subset(datasets.ImageFolder(root=str(DATA_ROOT), transform=train_tf), train_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(root=str(DATA_ROOT), transform=val_tf), val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_VAL,   shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5912e",
   "metadata": {},
   "source": [
    "## Peek a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "batch = next(iter(train_loader))\n",
    "imgs, labels = batch\n",
    "grid = torchvision.utils.make_grid(imgs[:32], nrow=8, padding=2, normalize=True)\n",
    "plt.figure(figsize=(10,8)); plt.imshow(grid.permute(1,2,0)); plt.axis(\"off\"); plt.title(\"Augmented training samples\"); plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
