{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5b4236",
   "metadata": {},
   "source": [
    "# 03 — Evaluate & Visualise\n",
    "\n",
    "Load a saved model checkpoint, compute **confusion matrix**, **classification report**, **top-1/top-5**, **F1 (macro)**, **mAP**, and visualise **feature maps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install numpy pandas scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import confusion_matrix, classification_report, top_k_accuracy_score, f1_score, average_precision_score\n",
    "\n",
    "DATA_ROOT = Path(\"/path/to/Places2_simp\")  # <-- EDIT\n",
    "CHECKPOINT = Path(\"../checkpoints/best_resnet34.pth\")\n",
    "BATCH_VAL, NUM_WORKERS = 1024, 4\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=str(DATA_ROOT), transform=val_tf)\n",
    "num_classes = len(dataset.classes)\n",
    "# Use same 80/20 split protocol as training:\n",
    "bare = datasets.ImageFolder(root=str(DATA_ROOT))\n",
    "targets = np.array([y for _, y in bare.imgs])\n",
    "val_idx = []\n",
    "for cls, idxs in pd.Series(np.arange(len(targets))).groupby(targets).groups.items():\n",
    "    idxs = np.array(list(idxs)); np.random.shuffle(idxs); n_val = int(0.2*len(idxs))\n",
    "    val_idx.extend(idxs[:n_val])\n",
    "val_ds = Subset(dataset, val_idx)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_VAL, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "def build_resnet34(num_classes):\n",
    "    m = models.resnet34(weights=None)\n",
    "    m.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(m.fc.in_features, num_classes))\n",
    "    return m\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = build_resnet34(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69742ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "all_probs, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x,y in val_loader:\n",
    "        x = x.to(device)\n",
    "        probs = torch.softmax(model(x), dim=1).cpu().numpy()\n",
    "        all_probs.append(probs); all_true.append(y.numpy())\n",
    "probs = np.concatenate(all_probs); y_true = np.concatenate(all_true)\n",
    "y_pred = probs.argmax(1)\n",
    "\n",
    "top1 = (y_pred == y_true).mean()\n",
    "top5 = top_k_accuracy_score(y_true, probs, k=5, labels=np.arange(num_classes))\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "y_true_ovr = np.eye(num_classes)[y_true]\n",
    "mAP = average_precision_score(y_true_ovr, probs, average=\"macro\")\n",
    "print(f\"Top-1: {top1*100:.2f}% | Top-5: {top5*100:.2f}% | F1-macro: {f1:.3f} | mAP: {mAP:.3f}\")\n",
    "print(classification_report(y_true, y_pred, target_names=dataset.classes)[:1000], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08977245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_classes))\n",
    "fig, ax = plt.subplots(figsize=(11,9))\n",
    "sns.heatmap(cm, ax=ax, cmap=\"Blues\", square=True, cbar=True)\n",
    "ax.set_title(\"Confusion Matrix — Validation\"); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "Path(\"../docs/images\").mkdir(parents=True, exist_ok=True)\n",
    "plt.tight_layout(); plt.savefig(\"../docs/images/confusion_val.png\", dpi=200); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature maps (first conv layer)\n",
    "hook_out = {}\n",
    "def hook_fn(m,i,o): hook_out['fm'] = o.detach().cpu()\n",
    "h = model.conv1.register_forward_hook(hook_fn)\n",
    "x,_ = next(iter(val_loader)); x=x[:8].to(device)\n",
    "with torch.no_grad(): _ = model(x)\n",
    "fm = hook_out['fm']\n",
    "N = min(8, fm.shape[1])\n",
    "fig, axes = plt.subplots(1,N, figsize=(2*N,2))\n",
    "for i in range(N):\n",
    "    axes[i].imshow(fm[0,i].numpy(), cmap='magma'); axes[i].axis('off')\n",
    "plt.suptitle(\"Feature maps — conv1\"); plt.tight_layout(); plt.savefig(\"../docs/images/featuremaps_conv1.png\", dpi=200); plt.show()\n",
    "h.remove()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
